{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bird Object Detection - Model Deployment\n",
    "\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This lets us make predictions (or inferences) from the model. Note that we donâ€™t have to host using the same type of instance that we used to train. Training is a prolonged and compute heavy job with different compute and memory requirements that hosting typically does not. In our case we chose the ml.p3.2xlarge instance to train, but we choose to host the model on the less expensive cpu instance, ml.m5.xlarge. The endpoint deployment takes several minutes, and can be accomplished with a single line of code calling the deploy method.\n",
    "\n",
    "Please find your training job name from the left-hand SageMaker Studio tab 'Components and Registries' and select 'Experiments and Trials'. Click on 'Unassigned trial components' and find your latest training job run. Click on it and expand details, where you will find the 'Job name' attribute.\n",
    "\n",
    "!![TrainingJobName](./images/JobName.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# this will create a 'default' sagemaker bucket if it doesn't exist (sagemaker-region-accountid)\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"DEMO-ObjectDetection-birds\"\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "TRAINING_JOB_NAME = \"<insert_training_job_name_here>\"\n",
    "\n",
    "od_estimator = Estimator.attach(TRAINING_JOB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realtime Prediction\n",
    "\n",
    "We will first create a real-time endpoint that will continuosly run and wait for inference (prediction) requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "object_detector = od_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download images from the types of birds we trained our model to recognize. These images have not been seen by the algorithm during training and are not part of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p unseen\n",
    "\n",
    "!wget -q -O unseen/multi-goldfinch-1.jpg https://t3.ftcdn.net/jpg/01/44/64/36/500_F_144643697_GJRUBtGc55KYSMpyg1Kucb9yJzvMQooW.jpg\n",
    "!wget -q -O unseen/northern-flicker-1.jpg https://upload.wikimedia.org/wikipedia/commons/5/5c/Northern_Flicker_%28Red-shafted%29.jpg\n",
    "!wget -q -O unseen/northern-cardinal-1.jpg https://cdn.pixabay.com/photo/2013/03/19/04/42/bird-94957_960_720.jpg\n",
    "!wget -q -O unseen/blue-jay-1.jpg https://cdn12.picryl.com/photo/2016/12/31/blue-jay-bird-feather-animals-b8ee04-1024.jpg\n",
    "!wget -q -O unseen/hummingbird-1.jpg http://res.freestockphotos.biz/pictures/17/17875-hummingbird-close-up-pv.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from our classes file, let's get the Class IDs for the specific bird classes we filtered out from our birds dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = \"CUB_200_2011/\"\n",
    "IMAGES_DIR = BASE_DIR + \"images/\"\n",
    "\n",
    "CLASSES_FILE = BASE_DIR + \"classes.txt\"\n",
    "CLASS_COLS = [\"class_number\", \"class_id\"]\n",
    "\n",
    "classes_df = pd.read_csv(CLASSES_FILE, sep=\" \", names=CLASS_COLS, header=None)\n",
    "\n",
    "CLASSES = [17, 36, 47, 68, 73]\n",
    "\n",
    "criteria = classes_df[\"class_number\"].isin(CLASSES)\n",
    "classes_df = classes_df[criteria]\n",
    "OBJECT_CATEGORIES = classes_df[\"class_id\"].values.tolist()\n",
    "\n",
    "print(OBJECT_CATEGORIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run inference for these images against our endpoint, we can see the details on the JSON response on the Object Detection SageMaker algorithm documentation pages here: https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-in-formats.html . You can see that the predictions will include **one or more** bounding box coordinates along with predicted label and confidence score.\n",
    "\n",
    "We threshold the predictions to only visualise those above a certain confidence score, that can be passed in the 'thresh' argument of our util **visualize_detection** helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker.serializers import IdentitySerializer\n",
    "import utils\n",
    "import json\n",
    "\n",
    "img_serializer = IdentitySerializer(content_type='image/jpeg')\n",
    "object_detector.serializer = img_serializer\n",
    "\n",
    "for filename in os.listdir('./unseen'):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        filepath = os.path.join('./unseen', filename)\n",
    "        with open(filepath, \"rb\") as image:\n",
    "            f = image.read()\n",
    "            img_data = bytearray(f)\n",
    "        print(\"Predicting class for {}\".format(filename))\n",
    "        dets = json.loads(object_detector.predict(img_data))\n",
    "        utils.visualize_detection(filepath, dets[\"prediction\"], OBJECT_CATEGORIES, thresh=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus!\n",
    "\n",
    "Play around with the threshold to see the types of bounding boxes that are returned at different confidence levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch prediction\n",
    "\n",
    "Let's now try to run our prediction not against a realtime endpoint but offline in a batch mmanner, where we send a batch of images to be inferred.\n",
    "\n",
    "The batch job can be started from our estimator, which contains the model artifact as well as the container reference used by SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = \"s3://{}/{}/batch_output\".format(bucket, prefix)\n",
    "\n",
    "batch_transform = od_estimator.transformer(\n",
    "    1,\n",
    "    \"ml.m5.xlarge\",\n",
    "    strategy=\"SingleRecord\",\n",
    "    assemble_with=None,\n",
    "    output_path=s3_output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "VAL_LST_FILE = \"birds_ssd_sample_val.lst\"\n",
    "BASE_DIR = \"CUB_200_2011/\"\n",
    "IMAGES_DIR = BASE_DIR + \"images/\"\n",
    "\n",
    "val_df = pd.read_csv(VAL_LST_FILE, sep=\"\\t\", header=None, names=[\"header_cols\", \"label_width\", \"zero_based_id\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"image_file_name\"])\n",
    "\n",
    "for image in val_df['image_file_name']:\n",
    "    \n",
    "    sess.upload_data(path=\"{}{}\".format(IMAGES_DIR, image), bucket=bucket, key_prefix=prefix + '/batch')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We launch the transform job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = \"s3://{}/{}\".format(bucket, prefix + '/batch')\n",
    "\n",
    "batch_transform.transform(input_batch, content_type='image/jpeg', split_type=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the output generated, note that we ran predictions on each image separately during our batch job, and as such we have a separate file for each image. There are options to aggregate all results within one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now let's view our predictions\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "input_files = s3_client.list_objects(Bucket=bucket,\n",
    "                               Prefix=prefix+'/batch_output')['Contents']\n",
    "for file in input_files:\n",
    "    print(file['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json('s3://{}/{}'.format(bucket, input_files[0]['Key']))"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (MXNet 1.8 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/mxnet-1.8-cpu-py37-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
