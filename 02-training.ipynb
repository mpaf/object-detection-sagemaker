{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bird Object Detection - Training\n",
    "\n",
    "Upload the training and validation data to the S3 bucket. We do this in multiple channels. Channels are simply directories in the bucket that differentiate the types of data provided to the algorithm. For the object detection algorithm, we call these directories train and validation.\n",
    "\n",
    "Next we define an output location in S3, where the model artifacts will be placed on completion of the training. These artifacts are the output of the algorithm’s traning job. We also get the URI to the Amazon SageMaker Object Detection docker image. This ensures the estimator uses the correct algorithm from the current region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# this will create a 'default' sagemaker bucket if it doesn't exist (sagemaker-region-accountid)\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "prefix = \"DEMO-ObjectDetection-birds\"\n",
    "TRAIN_REC_FILE = \"birds_ssd_sample_train.rec\"\n",
    "VAL_REC_FILE = \"birds_ssd_sample_val.rec\"\n",
    "TRAIN_LST_FILE = \"birds_ssd_sample_train.lst\"\n",
    "VAL_LST_FILE = \"birds_ssd_sample_val.lst\"\n",
    "\n",
    "# Upload the RecordIO files to train and validation channels\n",
    "train_channel = prefix + \"/train\"\n",
    "validation_channel = prefix + \"/validation\"\n",
    "\n",
    "sess.upload_data(path=\"birds_ssd_sample_train.rec\", bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path=\"birds_ssd_sample_val.rec\", bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = \"s3://{}/{}\".format(bucket, train_channel)\n",
    "s3_validation_data = \"s3://{}/{}\".format(bucket, validation_channel)\n",
    "\n",
    "print(s3_train_data)\n",
    "print(s3_validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data on S3, let's start a training job on SageMaker to train the model using the SageMaker Object Detection algoritm. The object detection algorithm at its core is the Single-Shot Multi-Box detection algorithm (SSD). This algorithm uses a base_network, which is typically a VGG or a ResNet. The Amazon SageMaker object detection algorithm supports VGG-16 and ResNet-50. It also has a number of hyperparameters that help configure the training job. The next step in our training, is to setup these hyperparameters and data channels for training the model. See the SageMaker Object Detection documentation for more details on its specific hyperparameters. \n",
    "\n",
    "We get the URI to the Amazon SageMaker Object Detection docker image. This ensures the estimator uses the correct algorithm from the current region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "training_image = image_uris.retrieve( \"object-detection\", sess.boto_region_name, version=\"1\")\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define an output location in S3, where the model artifacts will be placed on completion of the training. These artifacts are the output of the algorithm’s traning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = \"s3://{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our estimator object, that will be responsible for running the training job. We pass the container image used, the type and count of instances used in the training, attached storage volume size and also a maximum runtime parameter. We also set the training mode to 'File' and the output path for the resulting model. Note that 'File' will need to copy all the training data into the training instance(s) before training starts. Here we are using a GPU instance p3.2xlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_model = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    volume_size=50,\n",
    "    max_run=360000,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set the hyperparameters for our training job. Here we are using a pretrained 'resnet50' which returns a ResNet-50 network (a convolutional neural network that is 50 layers deep) trained on the ImageNet data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "IM2REC_SSD_COLS = [\n",
    "    \"header_cols\",\n",
    "    \"label_width\",\n",
    "    \"zero_based_id\",\n",
    "    \"xmin\",\n",
    "    \"ymin\",\n",
    "    \"xmax\",\n",
    "    \"ymax\",\n",
    "    \"image_file_name\",\n",
    "]\n",
    "\n",
    "train_lst = pd.read_csv(TRAIN_LST_FILE, sep=\"\\t\", names=IM2REC_SSD_COLS, header=None)\n",
    "\n",
    "num_classes = len(train_lst['zero_based_id'].unique())\n",
    "num_training_samples = train_lst.shape[0]\n",
    "num_epochs, lr_steps = (100, \"33,67\")\n",
    "\n",
    "od_model.set_hyperparameters(\n",
    "    base_network=\"resnet-50\",\n",
    "    use_pretrained_model=1,\n",
    "    num_classes=num_classes,\n",
    "    mini_batch_size=16,\n",
    "    epochs=num_epochs,\n",
    "    learning_rate=0.001,\n",
    "    lr_scheduler_step=lr_steps,\n",
    "    lr_scheduler_factor=0.1,\n",
    "    optimizer=\"sgd\",\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005,\n",
    "    overlap_threshold=0.5,\n",
    "    nms_threshold=0.45,\n",
    "    image_shape=512,\n",
    "    label_width=350,\n",
    "    num_training_samples=num_training_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we launch the training with our input training and test data (that was previously uploaded to S3). Note that we create SageMaker TrainingInput objects that specify the location of the datasets as well as their format (RecordIO) and the way we will copy the datasets to our training job instances (FullyReplicated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train_data = TrainingInput(\n",
    "    s3_train_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "validation_data = TrainingInput(\n",
    "    s3_validation_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "data_channels = {\"train\": train_data, \"validation\": validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we call fit() in our Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "od_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our model performance. In computer vision, mAP (Mean Average Precision) is a popular evaluation metric used for object detection (i.e. localisation and classification). Localization determines the location of an instance (e.g. bounding box coordinates) and classification tells you what it is (e.g. the species of a birds). Precision measures how accurate your predictions are. i.e. the percentage of your predictions are correct and is related to TRUE and FALSE positives in predictions. Object detection systems make predictions in terms of a bounding box and a class label. For each bounding box, we measure an overlap between the predicted bounding box and the ground truth bounding box. This is measured by **IoU (intersection over union)**.\n",
    "\n",
    "!![IoU](./images/IoU.png)\n",
    "\n",
    "\n",
    "For a prediction, we may get different binary TRUE or FALSE positives, by changing the IoU threshold, if IoU threshold is 0.5, and the IoU value for a prediction is 0.7, then we classify the prediction as True Positive (TP). On the other hand, if IoU is 0.3, we classify it as False Positive (FP). The mean Average Precision or mAP score is calculated by taking the mean AP over all classes and/or overall IoU thresholds. Using our validation data, we measured mAP for each epoch of our training job. We can plot it along the time axis to see that as it rapidly increases in the first epochs it eventually stabilizes and fluctuates around a certain mAP value. AP is always 0 < AP < 1, with higher values, meaning the model performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sagemaker.analytics import TrainingJobAnalytics\n",
    "\n",
    "training_job_name = '<insert_training_job_name>'\n",
    "metric_name = 'validation:mAP'\n",
    "\n",
    "metrics_dataframe = TrainingJobAnalytics(training_job_name=training_job_name,metric_names=[metric_name]).dataframe()\n",
    "plt = metrics_dataframe.plot(kind='line', figsize=(12,5), x='timestamp', y='value', style='b.', legend=False)\n",
    "plt.set_ylabel(metric_name);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous chart can also be visualized in SageMaker 'Trial Components'"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (MXNet 1.8 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/mxnet-1.8-cpu-py37-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
