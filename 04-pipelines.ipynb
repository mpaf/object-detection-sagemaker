{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bird Object Detection - SageMaker Pipelines\n",
    "\n",
    "Now that we have trained models and deployed them with SageMaker, we will look into how we can create parametrizable and reproducible ML pipelines with SageMaker Pipelines.\n",
    "\n",
    "Amazon SageMaker Model Building Pipelines offers machine learning (ML) application developers and operations engineers the ability to orchestrate SageMaker jobs and author reproducible ML pipelines. It also enables them to deploy custom-build models for inference in real-time with low latency, run offline inferences with Batch Transform, and track lineage of artifacts. They can institute sound operational practices in deploying and monitoring production workflows, deploying model artifacts, and tracking artifact lineage through a simple interface, adhering to safety and best practice paradigms for ML application development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# this will create a 'default' sagemaker bucket if it doesn't exist (sagemaker-region-accountid)\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "prefix = \"DEMO-ObjectDetection-birds\"\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline parameters\n",
    "\n",
    "Define Parameters to Parametrize Pipeline Execution\n",
    "\n",
    "The supported parameter types include:\n",
    "\n",
    "* ParameterString - represents a str Python type\n",
    "\n",
    "* ParameterInteger - represents an int Python type\n",
    "\n",
    "* ParameterFloat - represents a float Python type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "\n",
    "batch_data_uri = \"s3://{}/{}\".format(bucket, prefix + '/batch')\n",
    "\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.m5.large\"\n",
    ")\n",
    "training_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\", default_value=\"ml.p3.8xlarge\")\n",
    "\n",
    "model_approval_status = ParameterString(name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\")\n",
    "\n",
    "batch_data = ParameterString(name=\"BatchData\", default_value=batch_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Processing Step for Data Preparation\n",
    "\n",
    "Here we will use the code we work on the first notebook, data preparation, to launch in a SageMaker Processing job. Have a look at src/preprocess.py and note how it does the same things we did cell by cell - downloading the birds dataset and preparing the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet.estimator import MXNet\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "\n",
    "from sagemaker import image_uris\n",
    "\n",
    "training_image = image_uris.retrieve(\"mxnet\", sess.boto_region_name, instance_type=\"ml.m5.xlarge\", image_scope=\"training\", version=\"1.8\")\n",
    "\n",
    "mxnet_processor = FrameworkProcessor(\n",
    "    MXNet,\n",
    "    '1.8',\n",
    "    image_uri=training_image,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"mxnet-processor\",\n",
    "    role=role,\n",
    "    command=[\"python3\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is how we would run the processing job outside of the pipeline, note how the same parameters to run() are passed to the ProcessingStep\n",
    "\n",
    "```python\n",
    "mxnet_processor.run(\n",
    "    code='preprocess.py',\n",
    "    source_dir='src',\n",
    "    outputs=[ProcessingOutput(source='/opt/ml/processing/output/train'),\n",
    "        ProcessingOutput(source='/opt/ml/processing/output/validation')]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using the MXNet as a processing container in a pipeline, the ProcessingStep does not recognize the 'command' parameter of the FrameworkProcessor and does not change the entrypoing to python3 from /bin/bash. As such we need to do a couple of tricks, first we copy our python pre-processing script to a location in S3 (together with the utils functions). Then we define the entrypoint `code` to be our bash script that downloads those scripts from S3 and executes them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ./src/preprocess.py s3://$bucket/$prefix/pipeline_script/\n",
    "!aws s3 cp ./src/utils.py s3://$bucket/$prefix/pipeline_script/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a new 'cell magic' in this Jupyter Notebook that allows us to writefile with variables `write template` so that we can pass our bucket and prefix values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'w') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate src/preprocess_pipeline.sh\n",
    "#!/bin/bash\n",
    "aws s3 cp s3://{bucket}/{prefix}/pipeline_script/ . --recursive\n",
    "python3 preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to define our pipeline ProcessingStep. Note the parameter we have defined called **cache_config** this will allow Pipeline steps to cache their results and not run when the step parameters are similar across executions. This allows for example to quickly iterate on the training job parameters without needing to re-process the data. The cache is set to expire after a certain time, in this case, 30 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep, CacheConfig\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"BirdSetDataPreparation\",\n",
    "    processor=mxnet_processor,\n",
    "    cache_config = CacheConfig(enable_caching=True, expire_after='p30d'),\n",
    "    code='./src/preprocess_pipeline.sh',\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source='/opt/ml/processing/output/train'),\n",
    "        ProcessingOutput(output_name=\"validation\", source='/opt/ml/processing/output/validation')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Training Step\n",
    "\n",
    "Now let's use our Object Detection estimator to define a training step, that takes as inputs the outputs of the previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "training_image = image_uris.retrieve( \"object-detection\", sess.boto_region_name, version=\"1\")\n",
    "\n",
    "s3_output_location = \"s3://{}/{}/output\".format(bucket, prefix)\n",
    "\n",
    "num_epochs, lr_steps = (100, \"33,67\")\n",
    "\n",
    "od_model = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    volume_size=50,\n",
    "    max_run=3600,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess,\n",
    "    hyperparameters={\n",
    "        \"base_network\": \"resnet-50\",\n",
    "        \"use_pretrained_model\": 1,\n",
    "        \"num_classes\": 5,\n",
    "        \"mini_batch_size\": 16,\n",
    "        \"epochs\": 70,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"lr_scheduler_step\": lr_steps,\n",
    "        \"lr_scheduler_factor\": 0.1,\n",
    "        \"optimizer\": \"sgd\",\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 0.0005,\n",
    "        \"overlap_threshold\": 0.5,\n",
    "        \"nms_threshold\": 0.45,\n",
    "        \"image_shape\": 512,\n",
    "        \"label_width\": 350,\n",
    "        \"num_training_samples\": 238}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"AbaloneTrain\",\n",
    "    estimator=od_model,\n",
    "    cache_config = CacheConfig(enable_caching=True, expire_after='p30d'),\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"application/x-recordio\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri,\n",
    "            content_type=\"application/x-recordio\",\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally Let's define and create our pipeline\n",
    "\n",
    "Let's put together the steps before and create our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"ObjectDetectionPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        training_instance_type\n",
    "    ],\n",
    "    steps=[step_process, step_train]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start an execution here or from the SageMaker Studio Pipelines view. When starting an execution, we can also define the values for the parameters of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start(\n",
    "    parameters=dict(\n",
    "        ProcessingInstanceType=\"ml.c5.xlarge\"\n",
    "    )\n",
    ")\n",
    "\n",
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view and start pipeline executions from the Studio -> SageMaker Resources -> Pipelines -> ObjectDetectionPipeline.\n",
    "\n",
    "Since we enabled caching, if we now re-run the pipeline with a changed value for a Training job parametere, the processing step will re-use the outputs generated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start(\n",
    "    parameters=dict(\n",
    "        ProcessingInstanceType=\"ml.c5.xlarge\",\n",
    "        TrainingInstanceType=\"ml.p3.2xlarge\",\n",
    "    )\n",
    ")\n",
    "\n",
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineage Tracking\n",
    "\n",
    "Notice how the different steps of the pipeline are organised in 'Experiments and trials' tab as a separate 'Trial' and how lineage can be achieved in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sess)\n",
    "for execution_step in reversed(execution.list_steps()):\n",
    "    print(execution_step)\n",
    "    display(viz.show(pipeline_execution_step=execution_step))\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_train.properties.FinalMetricDataList.__dict__"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (MXNet 1.8 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/mxnet-1.8-cpu-py37-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
