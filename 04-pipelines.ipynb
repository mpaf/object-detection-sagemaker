{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bird Object Detection - SageMaker Pipelines\n",
    "\n",
    "Now that we have trained models and deployed them with SageMaker, we will look into how we can create parametrizable and reproducible ML pipelines with SageMaker Pipelines.\n",
    "\n",
    "Amazon SageMaker Model Building Pipelines offers machine learning (ML) application developers and operations engineers the ability to orchestrate SageMaker jobs and author reproducible ML pipelines. It also enables them to deploy custom-build models for inference in real-time with low latency, run offline inferences with Batch Transform, and track lineage of artifacts. They can institute sound operational practices in deploying and monitoring production workflows, deploying model artifacts, and tracking artifact lineage through a simple interface, adhering to safety and best practice paradigms for ML application development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# this will create a 'default' sagemaker bucket if it doesn't exist (sagemaker-region-accountid)\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "prefix = \"DEMO-ObjectDetection-birds\"\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline parameters\n",
    "\n",
    "Define Parameters to Parametrize Pipeline Execution\n",
    "\n",
    "The supported parameter types include:\n",
    "\n",
    "* ParameterString - represents a str Python type\n",
    "\n",
    "* ParameterInteger - represents an int Python type\n",
    "\n",
    "* ParameterFloat - represents a float Python type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "\n",
    "batch_data_uri = \"s3://{}/{}\".format(bucket, prefix + '/batch')\n",
    "\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.m5.large\"\n",
    ")\n",
    "training_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\", default_value=\"ml.p3.8xlarge\")\n",
    "\n",
    "batch_data_input = ParameterString(name=\"BatchDataInput\", default_value=batch_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Processing Step for Data Preparation\n",
    "\n",
    "Here we will use the code we work on the first notebook, data preparation, to launch in a SageMaker Processing job. Have a look at src/preprocess.py and note how it does the same things we did cell by cell - downloading the birds dataset and preparing the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet.estimator import MXNet\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "\n",
    "from sagemaker import image_uris\n",
    "\n",
    "training_image = image_uris.retrieve(\"mxnet\", sess.boto_region_name, instance_type=\"ml.m5.xlarge\", image_scope=\"training\", version=\"1.8\")\n",
    "\n",
    "mxnet_processor = FrameworkProcessor(\n",
    "    MXNet,\n",
    "    '1.8',\n",
    "    image_uri=training_image,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"mxnet-processor\",\n",
    "    role=role,\n",
    "    command=[\"python3\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is how we would run the processing job outside of the pipeline, note how the same parameters to run() are passed to the ProcessingStep\n",
    "\n",
    "```python\n",
    "mxnet_processor.run(\n",
    "    code='preprocess.py',\n",
    "    source_dir='src',\n",
    "    outputs=[ProcessingOutput(source='/opt/ml/processing/output/train'),\n",
    "        ProcessingOutput(source='/opt/ml/processing/output/validation')]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using the MXNet as a processing container in a pipeline, the ProcessingStep does not recognize the 'command' parameter of the FrameworkProcessor and does not change the entrypoing to python3 from /bin/bash. As such we need to do a couple of tricks, first we copy our python pre-processing script to a location in S3 (together with the utils functions). Then we define the entrypoint `code` to be our bash script that downloads those scripts from S3 and executes them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ./src/preprocess.py s3://$bucket/$prefix/pipeline_script/\n",
    "!aws s3 cp ./src/utils.py s3://$bucket/$prefix/pipeline_script/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a new 'cell magic' in this Jupyter Notebook that allows us to writefile with variables `write template` so that we can pass our bucket and prefix values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'w') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate src/preprocess_pipeline.sh\n",
    "#!/bin/bash\n",
    "aws s3 cp s3://{bucket}/{prefix}/pipeline_script/ . --recursive\n",
    "python3 preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to define our pipeline ProcessingStep. Note the parameter we have defined called **cache_config** this will allow Pipeline steps to cache their results and not run when the step parameters are similar across executions. This allows for example to quickly iterate on the training job parameters without needing to re-process the data. The cache is set to expire after a certain time, in this case, 30 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep, CacheConfig\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"BirdDetectionDataPreparation\",\n",
    "    processor=mxnet_processor,\n",
    "    cache_config = CacheConfig(enable_caching=True, expire_after='p30d'),\n",
    "    code='./src/preprocess_pipeline.sh',\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source='/opt/ml/processing/output/train'),\n",
    "        ProcessingOutput(output_name=\"validation\", source='/opt/ml/processing/output/validation')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Training Step\n",
    "\n",
    "Now let's use our Object Detection estimator to define a training step, that takes as inputs the outputs of the previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "training_image = image_uris.retrieve( \"object-detection\", sess.boto_region_name, version=\"1\")\n",
    "\n",
    "s3_output_location = \"s3://{}/{}/output\".format(bucket, prefix)\n",
    "\n",
    "num_epochs, lr_steps = (100, \"33,67\")\n",
    "\n",
    "od_model = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    volume_size=50,\n",
    "    max_run=3600,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess,\n",
    "    hyperparameters={\n",
    "        \"base_network\": \"resnet-50\",\n",
    "        \"use_pretrained_model\": 1,\n",
    "        \"num_classes\": 5,\n",
    "        \"mini_batch_size\": 16,\n",
    "        \"epochs\": 70,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"lr_scheduler_step\": lr_steps,\n",
    "        \"lr_scheduler_factor\": 0.1,\n",
    "        \"optimizer\": \"sgd\",\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 0.0005,\n",
    "        \"overlap_threshold\": 0.5,\n",
    "        \"nms_threshold\": 0.45,\n",
    "        \"image_shape\": 512,\n",
    "        \"label_width\": 350,\n",
    "        \"num_training_samples\": 238}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"BirdDetectionTrain\",\n",
    "    estimator=od_model,\n",
    "    cache_config = CacheConfig(enable_caching=True, expire_after='p30d'),\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"application/x-recordio\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri,\n",
    "            content_type=\"application/x-recordio\",\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And a Batch Transform job\n",
    "\n",
    "Let's move our 'unseen' bird pictures to S3, and trigger a Batch transform job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_image_input = sess.upload_data(path=\"./unseen/\", bucket=bucket, key_prefix=prefix + '/unseen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can 'deploy' our model in a Batch fashion, we need to register it as a model with SageMaker (currently it's only an S3 artifact that the Training Job returned. To register as a deployable model, we need to specify the allowed instance types and accelerator types to be used for model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "# we are using the same image for 'hosting' the model as we used for trianing it, since the image works for both use-cases\n",
    "model = Model(\n",
    "    image_uri=training_image,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "inputs = CreateModelInput(\n",
    "    instance_type=[\"ml.m5.large\", \"ml.m5.xlarge\", \"ml.c5.large\"],\n",
    "    accelerator_type=\"ml.eia1.medium\"\n",
    ")\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"BirdDetectionCreateModel\",\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now we create the Transformer and TransformStep. Note that transformer will use the image specified in our model object above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "# define a location for the Batch Job outputs\n",
    "batch_image_output = f\"s3://{bucket}/{prefix}/pipeline_batch_out\"\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=batch_image_output,\n",
    ")\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name=\"BirdDetectionBatchTransform\",\n",
    "    transformer=transformer,\n",
    "    inputs=TransformInput(data=batch_data_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally Let's define and create our pipeline\n",
    "\n",
    "Let's put together the steps before and create our pipeline. The steps provided don't need to be in order, as the dependencies between them will be resolved in the final Pipeline graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"BirdDetectionPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        training_instance_type,\n",
    "        batch_data_input\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_create_model, step_transform]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running our Pipeline\n",
    "\n",
    "If you navigate to the pipeline in SageMaker Studio 'Resources' tab -> 'Pipelines', you will see the below:\n",
    "\n",
    "![SageMaker Pipeline](./images/sm-pipeline.png)\n",
    "\n",
    "We can start an execution here or from the SageMaker Studio Pipelines view. When starting an execution, we can also define the values for the parameters of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start(\n",
    "    parameters=dict(\n",
    "        ProcessingInstanceType=\"ml.c5.xlarge\",\n",
    "        BatchDataInput=unseen_image_input,\n",
    "    )\n",
    ")\n",
    "\n",
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view and start pipeline executions from the Studio -> SageMaker Resources -> Pipelines -> BirdDetectionPipeline.\n",
    "\n",
    "Since we enabled caching, if we now re-run the pipeline with a changed value for a Training job parametere, the processing step will re-use the outputs generated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start(\n",
    "    parameters=dict(\n",
    "        ProcessingInstanceType=\"ml.c5.xlarge\",\n",
    "        BatchDataInput=unseen_image_input,\n",
    "        TrainingInstanceType=\"ml.p3.2xlarge\",\n",
    "    )\n",
    ")\n",
    "\n",
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Pipeline Results\n",
    "\n",
    "As before we can now review the results of our batch processing on the unseen images, hopefully with much better confidence scores due to the better learning_rate parameter pick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from src import utils\n",
    "import pandas as pd\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "prediction_files = s3_client.list_objects(Bucket=bucket,\n",
    "                               Prefix=prefix+'/pipeline_batch_out')['Contents']\n",
    "for prediction_file in prediction_files:\n",
    "\n",
    "    predictions = pd.read_json('s3://{}/{}'.format(bucket, prediction_file['Key']))\n",
    "\n",
    "    filename = prediction_file['Key'].rsplit('/', 1)[1].rsplit('.', 1)[0]\n",
    "\n",
    "    utils.visualize_detection(\n",
    "        './unseen/'+filename,\n",
    "        predictions['prediction'].tolist(),\n",
    "        [\"017.Cardinal\", \"036.Northern_Flicker\", \"047.American_Goldfinch\", \"068.Ruby_throated_Hummingbird\", \"073.Blue_Jay\"],\n",
    "        thresh=0.7\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineage Tracking\n",
    "\n",
    "Notice how the different steps of the pipeline are organised in 'Experiments and trials' tab as a separate 'Trial' and how lineage can be achieved in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sess)\n",
    "for execution_step in reversed(execution.list_steps()):\n",
    "    print(execution_step)\n",
    "    display(viz.show(pipeline_execution_step=execution_step))\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_train.properties.FinalMetricDataList.__dict__"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (MXNet 1.8 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/mxnet-1.8-cpu-py37-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
